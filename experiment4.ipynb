{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "from typing import List, Any\n",
    "import hashlib\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings, HuggingFacePipeline\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.25\n"
     ]
    }
   ],
   "source": [
    "import langchain\n",
    "print(langchain.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:19:05,513 - INFO - Use pytorch device_name: cpu\n",
      "2025-05-11 05:19:05,516 - INFO - Load pretrained SentenceTransformer: sentence-transformers/all-MiniLM-L6-v2\n",
      "2025-05-11 05:19:24,730 - INFO - Embedding model initialized successfully.\n",
      "2025-05-11 05:19:28,107 - INFO - Use pytorch device: cpu\n",
      "2025-05-11 05:19:28,671 - INFO - Reranker initialized successfully.\n"
     ]
    }
   ],
   "source": [
    "def initialize_embeddings(model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\") -> HuggingFaceEmbeddings:\n",
    "    \"\"\"Initialize Hugging Face embeddings.\"\"\"\n",
    "    try:\n",
    "        embeddings = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            multi_process=True,\n",
    "            encode_kwargs={\"normalize_embeddings\": True}\n",
    "        )\n",
    "        logger.info(\"Embedding model initialized successfully.\")\n",
    "        return embeddings\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing embedding model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def initialize_reranker(model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\") -> CrossEncoder:\n",
    "    \"\"\"Initialize cross-encoder reranker.\"\"\"\n",
    "    try:\n",
    "        reranker = CrossEncoder(model_name)\n",
    "        logger.info(\"Reranker initialized successfully.\")\n",
    "        return reranker\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing reranker: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Initialize models\n",
    "EMBEDDINGS = initialize_embeddings()\n",
    "RERANKER = initialize_reranker()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:19:34,444 - INFO - Loaded 10 pages from PDFs in data\n",
      "Splitting documents: 100%|██████████| 10/10 [00:00<00:00, 26.78it/s]\n",
      "2025-05-11 05:19:35,246 - INFO - Processed 19 unique document chunks (tokenizer-based)\n",
      "Splitting documents: 100%|██████████| 10/10 [00:00<00:00, 1231.23it/s]\n",
      "2025-05-11 05:19:35,279 - INFO - Processed 98 unique document chunks (character-based)\n",
      "2025-05-11 05:19:35,429 - INFO - Loading faiss with AVX2 support.\n",
      "2025-05-11 05:19:37,481 - INFO - Successfully loaded faiss with AVX2 support.\n",
      "2025-05-11 05:19:37,513 - INFO - Failed to load GPU Faiss: name 'GpuIndexIVFFlat' is not defined. Will not load constructor refs for GPU indexes. This is only an error if you're trying to use GPU Faiss.\n",
      "2025-05-11 05:19:37,613 - INFO - Loaded existing FAISS index at faiss_index_tokenizer\n",
      "2025-05-11 05:19:37,678 - INFO - Loaded existing FAISS index at faiss_index_char\n"
     ]
    }
   ],
   "source": [
    "def load_pdfs(directory: str = \"data\") -> List[LangchainDocument]:\n",
    "    \"\"\"Load all PDFs from a directory.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        logger.error(f\"Directory not found at {directory}\")\n",
    "        raise FileNotFoundError(f\"Directory not found at {directory}\")\n",
    "    try:\n",
    "        loader = PyPDFDirectoryLoader(directory)\n",
    "        documents = loader.load()\n",
    "        if not documents:\n",
    "            logger.warning(f\"No PDFs found in {directory}\")\n",
    "            raise ValueError(f\"No PDFs found in {directory}\")\n",
    "        logger.info(f\"Loaded {len(documents)} pages from PDFs in {directory}\")\n",
    "        return documents\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading PDFs: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def remove_duplicates(docs: List[LangchainDocument]) -> List[LangchainDocument]:\n",
    "    \"\"\"Remove duplicate documents based on content and metadata.\"\"\"\n",
    "    seen_hashes = set()\n",
    "    unique_docs = []\n",
    "    for doc in docs:\n",
    "        content_hash = hashlib.md5(doc.page_content.encode('utf-8')).hexdigest()\n",
    "        metadata_key = f\"{doc.metadata.get('source', '')}:{doc.metadata.get('page', '')}\"\n",
    "        unique_key = f\"{content_hash}:{metadata_key}\"\n",
    "        if unique_key not in seen_hashes:\n",
    "            seen_hashes.add(unique_key)\n",
    "            unique_docs.append(doc)\n",
    "    return unique_docs\n",
    "\n",
    "def split_documents(\n",
    "    documents: List[LangchainDocument],\n",
    "    chunk_size: int = 256,  # Optimized for CPU\n",
    "    use_tokenizer: bool = True,\n",
    "    tokenizer_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ") -> List[LangchainDocument]:\n",
    "    \"\"\"Split documents using RecursiveCharacterTextSplitter.\"\"\"\n",
    "    separators = [\n",
    "        \"\\n#{1,6} \",\n",
    "        \"```\\n\",\n",
    "        \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "        \"\\n---+\\n\",\n",
    "        \"\\n___+\\n\",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \"\",\n",
    "    ]\n",
    "    try:\n",
    "        if use_tokenizer:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "            splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
    "                tokenizer,\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=int(chunk_size / 10),\n",
    "                add_start_index=True,\n",
    "                strip_whitespace=True,\n",
    "                separators=separators,\n",
    "            )\n",
    "        else:\n",
    "            splitter = RecursiveCharacterTextSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=int(chunk_size / 10),\n",
    "                add_start_index=True,\n",
    "                strip_whitespace=True,\n",
    "                separators=separators,\n",
    "            )\n",
    "        \n",
    "        docs_processed = []\n",
    "        for doc in tqdm(documents, desc=\"Splitting documents\"):\n",
    "            docs_processed += splitter.split_documents([doc])\n",
    "        unique_docs = remove_duplicates(docs_processed)\n",
    "        logger.info(f\"Processed {len(unique_docs)} unique document chunks ({'tokenizer' if use_tokenizer else 'character'}-based)\")\n",
    "        return unique_docs\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error splitting documents: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_faiss_index(\n",
    "    documents: List[LangchainDocument],\n",
    "    embeddings: HuggingFaceEmbeddings,\n",
    "    index_path: str\n",
    ") -> FAISS:\n",
    "    \"\"\"Create or load FAISS index.\"\"\"\n",
    "    try:\n",
    "        if os.path.exists(index_path):\n",
    "            index = FAISS.load_local(index_path, embeddings, allow_dangerous_deserialization=True)\n",
    "            logger.info(f\"Loaded existing FAISS index at {index_path}\")\n",
    "            return index\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        index = FAISS.from_documents(documents, embeddings, distance_strategy=\"COSINE\")\n",
    "        os.makedirs(index_path, exist_ok=True)\n",
    "        index.save_local(index_path)\n",
    "        logger.info(f\"Created and saved FAISS index at {index_path}\")\n",
    "        return index\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating FAISS index: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Load and process documents\n",
    "DATA_DIR = \"data\"\n",
    "INDEX_PATH_TOKENIZER = \"faiss_index_tokenizer\"\n",
    "INDEX_PATH_CHAR = \"faiss_index_char\"\n",
    "\n",
    "RAW_DOCUMENTS = load_pdfs(DATA_DIR)\n",
    "DOCS_TOKENIZER = split_documents(RAW_DOCUMENTS, use_tokenizer=True)\n",
    "DOCS_CHAR = split_documents(RAW_DOCUMENTS, use_tokenizer=False)\n",
    "INDEX_TOKENIZER = create_faiss_index(DOCS_TOKENIZER, EMBEDDINGS, INDEX_PATH_TOKENIZER)\n",
    "INDEX_CHAR = create_faiss_index(DOCS_CHAR, EMBEDDINGS, INDEX_PATH_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "2025-05-11 05:19:41,267 - INFO - DistilGPT-2 initialized successfully.\n",
      "/tmp/ipykernel_5123/4080959199.py:31: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(\n",
      "2025-05-11 05:19:41,272 - INFO - Conversational RAG chain created successfully.\n",
      "2025-05-11 05:19:41,276 - INFO - Conversational RAG chain created successfully.\n"
     ]
    }
   ],
   "source": [
    "def initialize_llm(model_name: str = \"distilgpt2\") -> HuggingFacePipeline:\n",
    "    \"\"\"Initialize DistilGPT-2 LLM.\"\"\"\n",
    "    try:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "        pipe = pipeline(\n",
    "            \"text-generation\",\n",
    "            model=model,\n",
    "            tokenizer=tokenizer,\n",
    "            max_new_tokens=100,\n",
    "            do_sample=True,\n",
    "            temperature=0.7,\n",
    "            repetition_penalty=1.1,\n",
    "            device=-1\n",
    "        )\n",
    "        llm = HuggingFacePipeline(pipeline=pipe)\n",
    "        logger.info(\"DistilGPT-2 initialized successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error initializing DistilGPT-2: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def create_conversational_rag_chain(\n",
    "    vectorstore: FAISS,\n",
    "    reranker: CrossEncoder,\n",
    "    llm: HuggingFacePipeline,\n",
    "    k: int = 10,\n",
    "    top_n: int = 2\n",
    ") -> ConversationalRetrievalChain:\n",
    "    \"\"\"Create Conversational RAG chain with FAISS retriever and reranker.\"\"\"\n",
    "    memory = ConversationBufferMemory(\n",
    "        memory_key=\"chat_history\",\n",
    "        return_messages=True,\n",
    "        output_key=\"answer\",\n",
    "        max_token_limit=500\n",
    "    )\n",
    "    \n",
    "    prompt_template = PromptTemplate.from_template(\n",
    "        \"\"\"You are an expert on Adama Science and Technology University (ASTU) policies. Using only the provided context from ASTU policy documents, provide a concise and accurate answer to the question. Include the source document number if relevant. If the answer cannot be found in the context, respond with \"I don't know\" and nothing else. Do not generate information beyond the context. Consider the chat history for context.\n",
    "\n",
    "Chat History:\n",
    "{chat_history}\n",
    "Context:\n",
    "{context}\n",
    "---\n",
    "Question: {question}\"\"\"\n",
    "    )\n",
    "    \n",
    "    def format_docs(documents: List[LangchainDocument], query: str) -> str:\n",
    "        \"\"\"Rerank and format documents.\"\"\"\n",
    "        try:\n",
    "            if not documents:\n",
    "                return \"\"\n",
    "            pairs = [[query, doc.page_content] for doc in documents]\n",
    "            scores = reranker.predict(pairs, batch_size=32)\n",
    "            sorted_docs = [doc for _, doc in sorted(zip(scores, documents), key=lambda x: x[0], reverse=True)]\n",
    "            top_docs = sorted_docs[:top_n]\n",
    "            if not top_docs:\n",
    "                return \"\"\n",
    "            return \"\\n\".join([f\"Document {i}:::\\n{doc.page_content}\" for i, doc in enumerate(top_docs)])\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in format_docs: {str(e)}\")\n",
    "            return \"\"\n",
    "    \n",
    "    try:\n",
    "        retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "        \n",
    "        chain = ConversationalRetrievalChain.from_llm(\n",
    "            llm=llm,\n",
    "            retriever=retriever,\n",
    "            memory=memory,\n",
    "            combine_docs_chain_kwargs={\n",
    "                \"prompt\": prompt_template,\n",
    "                \"document_variable_name\": \"context\",\n",
    "                \"document_prompt\": PromptTemplate.from_template(\"Document {i}:::\\n{page_content}\")\n",
    "            },\n",
    "            return_source_documents=False,\n",
    "            verbose=False,\n",
    "            get_chat_history=lambda h: h,\n",
    "            chain_type=\"stuff\"\n",
    "        )\n",
    "        \n",
    "        # Wrap combine_docs_chain to include reranking\n",
    "        original_combine_docs = chain.combine_docs_chain\n",
    "        chain.combine_docs_chain = lambda docs, question, **kwargs: original_combine_docs(\n",
    "            docs=[LangchainDocument(page_content=format_docs(docs, question))], question=question, **kwargs\n",
    "        )\n",
    "        \n",
    "        logger.info(\"Conversational RAG chain created successfully.\")\n",
    "        return chain\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error creating Conversational RAG chain: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "# Initialize LLM and RAG chains\n",
    "LLM = initialize_llm()\n",
    "RAG_CHAIN_TOKENIZER = create_conversational_rag_chain(INDEX_TOKENIZER, RERANKER, LLM)\n",
    "RAG_CHAIN_CHAR = create_conversational_rag_chain(INDEX_CHAR, RERANKER, LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:19:41,327 - INFO - Processing query: What are ASTU’s add and drop policies? with Tokenizer-based splitting\n",
      "/tmp/ipykernel_5123/3488394275.py:6: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = rag_chain({\"question\": query})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Tokenizer-based Splitting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:19:41,672 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2025-05-11 05:19:41,673 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cafebc6d40424c2c9f7584161ae5262b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:23:19,112 - ERROR - Error processing query 'What are ASTU’s add and drop policies?': 'function' object has no attribute 'run'\n",
      "2025-05-11 05:23:19,113 - INFO - Processing query: Tell me more about the deadlines. with Tokenizer-based splitting\n",
      "2025-05-11 05:23:19,116 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2025-05-11 05:23:19,117 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'run'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b0f8e9f4d447e3b7d84befcf7d09b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:24:16,468 - ERROR - Error processing query 'Tell me more about the deadlines.': 'function' object has no attribute 'run'\n",
      "2025-05-11 05:24:16,471 - INFO - Processing query: What does Article 82 say about student conduct? with Tokenizer-based splitting\n",
      "2025-05-11 05:24:16,476 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2025-05-11 05:24:16,479 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'run'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8069ad5a03754627a84d9aabd212c5af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:24:53,271 - ERROR - Error processing query 'What does Article 82 say about student conduct?': 'function' object has no attribute 'run'\n",
      "2025-05-11 05:24:53,273 - INFO - Processing query: What is the capital of France? with Tokenizer-based splitting\n",
      "2025-05-11 05:24:53,277 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2025-05-11 05:24:53,278 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'run'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ed93eff6266436f87139a6571677f2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:25:36,353 - ERROR - Error processing query 'What is the capital of France?': 'function' object has no attribute 'run'\n",
      "2025-05-11 05:25:36,356 - INFO - Tokenizer-based chat history cleared\n",
      "2025-05-11 05:25:36,357 - INFO - Processing query: What are ASTU’s add and drop policies? with Character-based splitting\n",
      "2025-05-11 05:25:36,359 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2025-05-11 05:25:36,363 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'run'\n",
      "\n",
      "Testing Character-based Splitting\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c99d79df37b94d508aaccf0b5052ee7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:26:14,462 - ERROR - Error processing query 'What are ASTU’s add and drop policies?': 'function' object has no attribute 'run'\n",
      "2025-05-11 05:26:14,463 - INFO - Processing query: Tell me more about the deadlines. with Character-based splitting\n",
      "2025-05-11 05:26:14,465 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2025-05-11 05:26:14,467 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'run'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea42110dd369419eba06fb0cf56ab209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:26:51,755 - ERROR - Error processing query 'Tell me more about the deadlines.': 'function' object has no attribute 'run'\n",
      "2025-05-11 05:26:51,757 - INFO - Processing query: What does Article 82 say about student conduct? with Character-based splitting\n",
      "2025-05-11 05:26:51,761 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2025-05-11 05:26:51,764 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'run'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13b93500384241febde05dfd01eea169",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:27:28,645 - ERROR - Error processing query 'What does Article 82 say about student conduct?': 'function' object has no attribute 'run'\n",
      "2025-05-11 05:27:28,646 - INFO - Processing query: What is the capital of France? with Character-based splitting\n",
      "2025-05-11 05:27:28,651 - INFO - CUDA/NPU is not available. Starting 4 CPU workers\n",
      "2025-05-11 05:27:28,654 - INFO - Start multi-process pool on devices: cpu, cpu, cpu, cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'run'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b87e1445cdb483c8a9961d92125fcbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-11 05:28:09,342 - ERROR - Error processing query 'What is the capital of France?': 'function' object has no attribute 'run'\n",
      "2025-05-11 05:28:09,344 - INFO - Character-based chat history cleared\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: 'function' object has no attribute 'run'\n"
     ]
    }
   ],
   "source": [
    "def test_rag_chain(rag_chain: ConversationalRetrievalChain, queries: List[str], method: str) -> None:\n",
    "    \"\"\"Test RAG chain with a list of queries and print results.\"\"\"\n",
    "    for query in queries:\n",
    "        logger.info(f\"Processing query: {query} with {method} splitting\")\n",
    "        try:\n",
    "            result = rag_chain({\"question\": query})\n",
    "            print(f\"\\nMethod: {method}\")\n",
    "            print(f\"Query: {query}\")\n",
    "            print(f\"Answer: {result['answer']}\")\n",
    "            print(f\"Chat History: {rag_chain.memory.buffer}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error processing query '{query}': {str(e)}\")\n",
    "            print(f\"Error: {str(e)}\")\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What are ASTU’s add and drop policies?\",\n",
    "    \"Tell me more about the deadlines.\",\n",
    "    \"What does Article 82 say about student conduct?\",\n",
    "    \"What is the capital of France?\"\n",
    "]\n",
    "\n",
    "# Test tokenizer-based chain\n",
    "print(\"Testing Tokenizer-based Splitting\")\n",
    "test_rag_chain(RAG_CHAIN_TOKENIZER, test_queries, \"Tokenizer-based\")\n",
    "\n",
    "# Reset memory\n",
    "RAG_CHAIN_TOKENIZER.memory.clear()\n",
    "logger.info(\"Tokenizer-based chat history cleared\")\n",
    "\n",
    "# Test character-based chain\n",
    "print(\"\\nTesting Character-based Splitting\")\n",
    "test_rag_chain(RAG_CHAIN_CHAR, test_queries, \"Character-based\")\n",
    "\n",
    "# Reset memory\n",
    "RAG_CHAIN_CHAR.memory.clear()\n",
    "logger.info(\"Character-based chat history cleared\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
