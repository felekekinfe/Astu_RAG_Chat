{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader,PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter,TokenTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Hugging Face embeddings\n",
    "EMBEDDING_MODEL_NAME ='sentence-transformers/all-MiniLM-L6-v2'\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    encode_kwargs={\"normalize_embeddings\": True}  # For cosine similarity\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize lightweight reranker\n",
    "RERANKER_MODEL_NAME = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"\n",
    "reranker = CrossEncoder(RERANKER_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized embedding model and tokenizer.\n",
      "FAISS index not found. Creating a new one...\n",
      "Loaded PDF with 10 pages.\n",
      "Split into 10 document chunks.\n",
      "Created and saved new FAISS index at /home/cs/Desktop/Project/Astu_RAG_Chat/faiss_index.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Configuration\n",
    "PDF_PATH = os.path.abspath(\"data\")  # Absolute path\n",
    "FAISS_INDEX_PATH = os.path.abspath(\"faiss_index\")  # Absolute path\n",
    "EMBEDDING_MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CHUNK_SIZE = 1000\n",
    "CHUNK_OVERLAP = 200\n",
    "\n",
    "# Initialize embedding model and tokenizer\n",
    "try:\n",
    "    # Use HuggingFaceEmbeddings instead of SentenceTransformer\n",
    "    embedding_model = HuggingFaceEmbeddings(\n",
    "        model_name=EMBEDDING_MODEL_NAME,\n",
    "        model_kwargs={\"device\": \"cpu\"},  # Adjust to \"cuda\" if GPU is available\n",
    "        encode_kwargs={\"normalize_embeddings\": True}\n",
    "    )\n",
    "    tokenizer = AutoTokenizer.from_pretrained(EMBEDDING_MODEL_NAME)\n",
    "    print(\"Initialized embedding model and tokenizer.\")\n",
    "except Exception as e:\n",
    "    raise Exception(f\"Failed to load embedding model or tokenizer: {str(e)}\")\n",
    "\n",
    "# Load or create FAISS vector database\n",
    "if os.path.exists(os.path.join(FAISS_INDEX_PATH, \"index.faiss\")):\n",
    "    try:\n",
    "        KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(\n",
    "            FAISS_INDEX_PATH, embedding_model, allow_dangerous_deserialization=True\n",
    "        )\n",
    "        print(\"Loaded existing FAISS index.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load FAISS index: {str(e)}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"FAISS index not found. Creating a new one...\")\n",
    "    # Verify PDF exists\n",
    "    if not os.path.exists(PDF_PATH):\n",
    "        raise FileNotFoundError(f\"PDF file not found at {PDF_PATH}\")\n",
    "\n",
    "    # Load PDF\n",
    "    try:\n",
    "        loader = PyPDFDirectoryLoader(PDF_PATH)\n",
    "        RAW_KNOWLEDGE_BASE = loader.load()\n",
    "        print(f\"Loaded PDF with {len(RAW_KNOWLEDGE_BASE)} pages.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to load PDF: {str(e)}\")\n",
    "\n",
    "    # Initialize text splitter\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=CHUNK_SIZE,\n",
    "        chunk_overlap=CHUNK_OVERLAP,\n",
    "        length_function=lambda x: len(tokenizer.encode(x, add_special_tokens=False)),\n",
    "        separators=[\n",
    "        \"\\n#{1,6} \",\n",
    "        \"```\\n\",\n",
    "        \"\\n\\\\*\\\\*\\\\*+\\n\",\n",
    "        \"\\n---+\\n\",\n",
    "        \"\\n___+\\n\",\n",
    "        \"\\n\\n\",\n",
    "        \"\\n\",\n",
    "        \" \",\n",
    "        \".\",\n",
    "    ]\n",
    "    )\n",
    "\n",
    "    # Split documents\n",
    "    try:\n",
    "        docs_processed = text_splitter.split_documents(RAW_KNOWLEDGE_BASE)\n",
    "        print(f\"Split into {len(docs_processed)} document chunks.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to split documents: {str(e)}\")\n",
    "\n",
    "    # Create and save FAISS index\n",
    "    try:\n",
    "        KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n",
    "            docs_processed, embedding_model, distance_strategy=\"COSINE\"\n",
    "        )\n",
    "        os.makedirs(FAISS_INDEX_PATH, exist_ok=True)  # Ensure directory exists\n",
    "        KNOWLEDGE_VECTOR_DATABASE.save_local(FAISS_INDEX_PATH)\n",
    "        print(f\"Created and saved new FAISS index at {FAISS_INDEX_PATH}.\")\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to create/save FAISS index: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# Initialize DistilGPT-2\n",
    "READER_MODEL_NAME = \"distilgpt2\"\n",
    "READER_LLM = pipeline(\n",
    "    task=\"text-generation\",\n",
    "    model=READER_MODEL_NAME,\n",
    "    tokenizer=AutoTokenizer.from_pretrained(READER_MODEL_NAME),\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    repetition_penalty=1.1,\n",
    "    max_new_tokens=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving top 5 documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Both `max_new_tokens` (=100) and `max_length`(=1000) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Selected 2 documents for context.\n",
      "=> Generating answer...\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from sentence_transformers import CrossEncoder\n",
    "from transformers import pipeline\n",
    "import numpy as np\n",
    "\n",
    "# Define RAG prompt template\n",
    "# Define optimized RAG prompt\n",
    "prompt_in_chat_format = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": \"\"\"You are an expert on Adama Science and Technology University (ASTU) policies. Using only the provided context from ASTU policy documents, provide a concise and accurate answer to the question. Include the source document number if relevant. If the answer cannot be found in the context, respond with \"I don't know\" and nothing else. Do not generate information beyond the context.\"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"\"\"Context:\n",
    "{context}\n",
    "---\n",
    "Question: {question}\"\"\"\n",
    "    },\n",
    "]\n",
    "RAG_PROMPT_TEMPLATE = PromptTemplate.from_template(\n",
    "    \"\"\"{system}\\n\\n{user}\"\"\".format(\n",
    "        system=prompt_in_chat_format[0][\"content\"],\n",
    "        user=prompt_in_chat_format[1][\"content\"]\n",
    "    )\n",
    ")\n",
    "\n",
    "def answer_with_rag(\n",
    "    question: str,\n",
    "    llm: pipeline,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: CrossEncoder,\n",
    "    num_retrieved_docs: int = 5,\n",
    "    num_docs_final: int = 2\n",
    "):\n",
    "   \n",
    "    try:\n",
    "        # Validate inputs\n",
    "        if not isinstance(knowledge_index, FAISS):\n",
    "            raise ValueError(\"knowledge_index must be a FAISS vector store\")\n",
    "        if not isinstance(reranker, CrossEncoder):\n",
    "            raise ValueError(\"reranker must be a CrossEncoder model\")\n",
    "        if not question.strip():\n",
    "            raise ValueError(\"Question cannot be empty\")\n",
    "\n",
    "        # Retrieve documents\n",
    "        print(f\"=> Retrieving top {num_retrieved_docs} documents...\")\n",
    "        relevant_docs = knowledge_index.similarity_search_with_score(query=question, k=num_retrieved_docs)\n",
    "        \n",
    "        # Extract documents and scores (convert cosine distance to similarity)\n",
    "        docs_with_scores = [(doc.page_content, 1 - score) for doc, score in relevant_docs]  # Convert distance to similarity\n",
    "        doc_texts = [doc for doc, _ in docs_with_scores]\n",
    "        doc_scores = [score for _, score in docs_with_scores]\n",
    "        \n",
    "        # Rerank documents\n",
    "        print(\"=> Reranking documents...\")\n",
    "        pairs = [[question, doc] for doc in doc_texts]\n",
    "        rerank_scores = reranker.predict(pairs)\n",
    "        \n",
    "        # Normalize rerank scores (if not already normalized)\n",
    "        rerank_scores = (rerank_scores - np.min(rerank_scores)) / (np.max(rerank_scores) - np.min(rerank_scores) + 1e-10)\n",
    "        \n",
    "        # Combine scores (higher is better)\n",
    "        combined_scores = [0.7* rerank_score + 0.3 * doc_score for rerank_score, doc_score in zip(rerank_scores, doc_scores)]\n",
    "        sorted_docs = [doc for _, doc in sorted(zip(combined_scores, doc_texts), reverse=True)]\n",
    "        \n",
    "        # Select top documents\n",
    "        selected_docs = sorted_docs[:min(num_docs_final, len(sorted_docs))]\n",
    "        print(f\"=> Selected {len(selected_docs)} documents for context.\")\n",
    "        \n",
    "        # Build context\n",
    "        context = \"\\nExtracted documents:\\n\"\n",
    "        context += \"\".join([f\"Document {str(i)}:::\\n{doc}\\n\" for i, doc in enumerate(selected_docs)])\n",
    "        \n",
    "        # Generate answer\n",
    "        print(\"=> Generating answer...\")\n",
    "        final_prompt = RAG_PROMPT_TEMPLATE.format(question=question, context=context)\n",
    "        answer = llm(final_prompt, max_length=1000, num_return_sequences=10, temperature=0.5)[0][\"generated_text\"]\n",
    "        \n",
    "        return answer.strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in RAG pipaeline: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "query = \"tell me about addmission\"\n",
    "answer = answer_with_rag(query, READER_LLM, KNOWLEDGE_VECTOR_DATABASE, reranker)\n",
    "print(f\"Answer: {answer}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.0",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
